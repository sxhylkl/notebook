### SKlearn_Learn-0.19版本的API

---
<font size=4>[1.sklearn.base: 基类和实用函数](#1)</font><br>
<font size=4>[2.sklearn.calibration: 概率检验](#2)</font><br>
<font size=4>[3.sklearn.cluster: 聚类](#3)</font><br>
<font size=4>[4.sklearn.cluster.bicluster: 双向聚类](#4)</font><br>
<font size=4>[5.sklearn.covariance: 协方差估计](#5)</font><br>
<font size=4>[6.sklearn.cross_decomposition: 交叉分解](#6)</font><br>
<font size=4>[7.sklearn.datasets: 数据集](#7)</font><br>
<font size=4>[8.sklearn.decomposition: 矩阵分解](#8)</font><br>
<font size=4>[9.sklearn.discriminant_analysis: 判别分析](#9)</font><br>
<font size=4>[10.sklearn.dummy: 虚拟估计器](#10)</font><br>
<font size=4>[11.sklearn.ensemble: 集成方法](#11)</font><br>
<font size=4>[12.sklearn.exceptions: 异常和警告](#12)</font><br>
<font size=4>[13.sklearn.feature_extraction: 特征提取](#13)</font><br>
<font size=4>[14.sklearn.feature_selection: 特征选择](#14)</font><br>
<font size=4>[15.sklearn.gaussian_process: 高斯过程](#15)</font><br>
<font size=4>[16.sklearn.isotonic: 保序回归](#16)</font><br>
<font size=4>[17.sklearn.kernel_approximation 核近似](#17)</font><br>
<font size=4>[18.sklearn.kernel_ridge 内核岭回归](#18)</font><br>
<font size=4>[19.sklearn.linear_model: 广义线性模型](#19)</font><br>
<font size=4>[20.sklearn.manifold: 集成学习](#20)</font><br>
<font size=4>[21.sklearn.metrics: 指标](#21)</font><br>
<font size=4>[22.sklearn.mixture: 高斯混合模型](#22)</font><br>
<font size=4>[23.sklearn.model_selection: 模型选择](#23)</font><br>
<font size=4>[24.sklearn.multiclass: 多类和多标签分类](#24)</font><br>
<font size=4>[25.sklearn.multioutput: 多输出回归和分类](#25)</font><br>
<font size=4>[26.sklearn.naive_bayes: 朴素贝叶斯](#26)</font><br>
<font size=4>[27.sklearn.neighbors: 最近邻](#27)</font><br>
<font size=4>[28.sklearn.neural_network: 神经网络模型](#28)</font><br>
<font size=4>[29.sklearn.pipeline: 管道线](#29)</font><br>
<font size=4>[30.sklearn.preprocessing: 预处理和正则化](#30)</font><br>
<font size=4>[31.sklearn.random_projection: 随机投影](#31)</font><br>
<font size=4>[32.sklearn.semi_supervised 半监督学习](#32)</font><br>
<font size=4>[33.sklearn.svm: 支持向量机](#33)</font><br>
<font size=4>[34.sklearn.tree: 决策树](#34)</font><br>
<font size=4>[35.sklearn.utils: 效率](#35)</font><br>
<font size=4>[36.最近弃用](#36)</font><br>

---

<h4 id="1">1.sklearn.base: 基类和实用函数</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="2">2.sklearn.calibration: 概率检验</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="3">3.sklearn.cluster: 聚类</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="4">4.sklearn.cluster.bicluster: 双向聚类</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="5">5.sklearn.covariance: 协方差估计</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="6">6.sklearn.cross_decomposition: 交叉分解</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="7">7.sklearn.datasets: 数据集</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="8">8.sklearn.decomposition: 矩阵分解</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="9">9.sklearn.discriminant_analysis: 判别分析</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="10">10.sklearn.dummy: 虚拟估计器</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="11">11.sklearn.ensemble: 集成方法</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="12">12.sklearn.exceptions: 异常和警告</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="13">13.sklearn.feature_extraction: 特征提取</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="14">14.sklearn.feature_selection: 特征选择</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="15">15.sklearn.gaussian_process: 高斯过程</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="16">16.sklearn.isotonic: 保序回归</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="17">17.sklearn.kernel_approximation 核近似</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="18">18.sklearn.kernel_ridge 内核岭回归</h4>

```python
brew install 软件名
import numpy as np
```
<h4 id="19">19.sklearn.linear_model: 广义线性模型</h4>

```python
brew install 软件名
import numpy as np
```
<h4 id="20">20.sklearn.manifold: 集成学习</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="21">21.sklearn.metrics: 指标</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="22">22.sklearn.mixture: 高斯混合模型</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="23">23.sklearn.model_selection: 模型选择</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="24">24.sklearn.multiclass: 多类和多标签分类</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="25">25.sklearn.multioutput: 多输出回归和分类</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="26">26.sklearn.naive_bayes: 朴素贝叶斯</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="27">27.sklearn.neighbors: 最近邻</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="28">28.sklearn.neural_network: 神经网络模型</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="29">29.sklearn.pipeline: 管道线</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="30">30.sklearn.preprocessing: 预处理和正则化</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="31">31.sklearn.random_projection: 随机投影</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="32">32.sklearn.semi_supervised 半监督学习</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="33">33.sklearn.svm: 支持向量机</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="34">34.sklearn.tree: 决策树</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="35">35.sklearn.utils: 效率</h4>

```python
brew install 软件名
import numpy as np
```

<h4 id="36">36.最近弃用</h4>

```python
brew install 软件名
import numpy as np
```


##### sklearn.base: 基类和实用函数
Base classes for all estimators.

##### 基类
base.BaseEstimator	Base class for all estimators in scikit-learn
base.ClassifierMixin	Mixin class for all classifiers in scikit-learn.
base.ClusterMixin	Mixin class for all cluster estimators in scikit-learn.
base.RegressorMixin	Mixin class for all regression estimators in scikit-learn.
base.TransformerMixin	Mixin class for all transformers in scikit-learn.
##### 函数
base.clone(estimator[, safe])	Constructs a new estimator with the same parameters.
config_context(*args, **kwds)	Context manager for global scikit-learn configuration
get_config()	Retrieve current values for configuration set by set_config
set_config([assume_finite])	Set global scikit-learn configuration
sklearn.calibration: 概率检验
Calibration of predicted probabilities.

##### 用户指南: 参阅 概率校准 章节来查阅更多相关内容。

calibration.CalibratedClassifierCV([…])	Probability calibration with isotonic regression or sigmoid.
calibration.calibration_curve(y_true, y_prob)	Compute true and predicted probabilities for a calibration curve.
sklearn.cluster: 聚类
The sklearn.cluster module gathers popular unsupervised clustering algorithms.

##### 用户指南: 参阅 聚类 章节来查阅更多内容。

##### 类
cluster.AffinityPropagation([damping, …])	Perform Affinity Propagation Clustering of data.
cluster.AgglomerativeClustering([…])	Agglomerative Clustering
cluster.Birch([threshold, branching_factor, …])	Implements the Birch clustering algorithm.
cluster.DBSCAN([eps, min_samples, metric, …])	Perform DBSCAN clustering from vector array or distance matrix.
cluster.FeatureAgglomeration([n_clusters, …])	Agglomerate features.
cluster.KMeans([n_clusters, init, n_init, …])	K-Means clustering
cluster.MiniBatchKMeans([n_clusters, init, …])	Mini-Batch K-Means clustering
cluster.MeanShift([bandwidth, seeds, …])	Mean shift clustering using a flat kernel.
cluster.SpectralClustering([n_clusters, …])	Apply clustering to a projection to the normalized laplacian.
##### 函数

cluster.affinity_propagation(S[, …])	Perform Affinity Propagation Clustering of data
cluster.dbscan(X[, eps, min_samples, …])	Perform DBSCAN clustering from vector array or distance matrix.
cluster.estimate_bandwidth(X[, quantile, …])	Estimate the bandwidth to use with the mean-shift algorithm.
cluster.k_means(X, n_clusters[, init, …])	K-means clustering algorithm.
cluster.mean_shift(X[, bandwidth, seeds, …])	Perform mean shift clustering of data using a flat kernel.
cluster.spectral_clustering(affinity[, …])	Apply clustering to a projection to the normalized laplacian.
cluster.ward_tree(X[, connectivity, …])	Ward clustering based on a Feature matrix.
sklearn.cluster.bicluster: 双向聚类
Spectral biclustering algorithms.

Authors : Kemal Eren License: BSD 3 clause

用户指南: 参阅 双聚类 章节获取更多内容。

##### 类
SpectralBiclustering([n_clusters, method, …])	Spectral biclustering (Kluger, 2003).
SpectralCoclustering([n_clusters, …])	Spectral Co-Clustering algorithm (Dhillon, 2001).
sklearn.covariance: 协方差估计
The sklearn.covariance module includes methods and algorithms to robustly estimate the covariance of features given a set of points. The precision matrix defined as the inverse of the covariance is also estimated. Covariance estimation is closely related to the theory of Gaussian Graphical Models.

用户指南 : 参阅 covariance 章节来获取更多内容。

covariance.EmpiricalCovariance([…])	Maximum likelihood covariance estimator
covariance.EllipticEnvelope([…])	An object for detecting outliers in a Gaussian distributed dataset.
covariance.GraphLasso([alpha, mode, tol, …])	Sparse inverse covariance estimation with an l1-penalized estimator.
covariance.GraphLassoCV([alphas, …])	Sparse inverse covariance w/ cross-validated choice of the l1 penalty
covariance.LedoitWolf([store_precision, …])	LedoitWolf Estimator
covariance.MinCovDet([store_precision, …])	Minimum Covariance Determinant (MCD): robust estimator of covariance.
covariance.OAS([store_precision, …])	Oracle Approximating Shrinkage Estimator
covariance.ShrunkCovariance([…])	Covariance estimator with shrinkage
covariance.empirical_covariance(X[, …])	Computes the Maximum likelihood covariance estimator
covariance.graph_lasso(emp_cov, alpha[, …])	l1-penalized covariance estimator
covariance.ledoit_wolf(X[, assume_centered, …])	Estimates the shrunk Ledoit-Wolf covariance matrix.
covariance.oas(X[, assume_centered])	Estimate covariance with the Oracle Approximating Shrinkage algorithm.
covariance.shrunk_covariance(emp_cov[, …])	Calculates a covariance matrix shrunk on the diagonal
sklearn.cross_decomposition: 交叉分解
用户指南: 参阅 交叉分解 章节来获取更多内容。

cross_decomposition.CCA([n_components, …])	CCA Canonical Correlation Analysis.
cross_decomposition.PLSCanonical([…])	PLSCanonical implements the 2 blocks canonical PLS of the original Wold algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].
cross_decomposition.PLSRegression([…])	PLS regression
cross_decomposition.PLSSVD([n_components, …])	Partial Least Square SVD
sklearn.datasets: 数据集
The sklearn.datasets module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.

用户指南： 参阅 数据集加载工具 章节来获取更多内容。

##### 数据装载
datasets.clear_data_home([data_home])	Delete all the content of the data home cache.
datasets.dump_svmlight_file(X, y, f[, …])	Dump the dataset in svmlight / libsvm file format.
datasets.fetch_20newsgroups([data_home, …])	Load the filenames and data from the 20 newsgroups dataset.
datasets.fetch_20newsgroups_vectorized([…])	Load the 20 newsgroups dataset and transform it into tf-idf vectors.
datasets.fetch_california_housing([…])	Loader for the California housing dataset from StatLib.
datasets.fetch_covtype([data_home, …])	Load the covertype dataset, downloading it if necessary.
datasets.fetch_kddcup99([subset, data_home, …])	Load and return the kddcup 99 dataset (classification).
datasets.fetch_lfw_pairs([subset, …])	Loader for the Labeled Faces in the Wild (LFW) pairs dataset
datasets.fetch_lfw_people([data_home, …])	Loader for the Labeled Faces in the Wild (LFW) people dataset
datasets.fetch_mldata(dataname[, …])	Fetch an mldata.org data set
datasets.fetch_olivetti_faces([data_home, …])	Loader for the Olivetti faces data-set from AT&T.
datasets.fetch_rcv1([data_home, subset, …])	Load the RCV1 multilabel dataset, downloading it if necessary.
datasets.fetch_species_distributions([…])	Loader for species distribution dataset from Phillips et.
datasets.get_data_home([data_home])	Return the path of the scikit-learn data dir.
datasets.load_boston([return_X_y])	Load and return the boston house-prices dataset (regression).
datasets.load_breast_cancer([return_X_y])	Load and return the breast cancer wisconsin dataset (classification).
datasets.load_diabetes([return_X_y])	Load and return the diabetes dataset (regression).
datasets.load_digits([n_class, return_X_y])	Load and return the digits dataset (classification).
datasets.load_files(container_path[, …])	Load text files with categories as subfolder names.
datasets.load_iris([return_X_y])	Load and return the iris dataset (classification).
datasets.load_linnerud([return_X_y])	Load and return the linnerud dataset (multivariate regression).
datasets.load_mlcomp(*args, **kwargs)	DEPRECATED: since the http://mlcomp.org/ website will shut down in March 2017, the load_mlcomp function was deprecated in version 0.19 and will be removed in 0.21.
datasets.load_sample_image(image_name)	Load the numpy array of a single sample image
datasets.load_sample_images()	Load sample images for image manipulation.
datasets.load_svmlight_file(f[, n_features, …])	Load datasets in the svmlight / libsvm format into sparse CSR matrix
datasets.load_svmlight_files(files[, …])	Load dataset from multiple files in SVMlight format
datasets.load_wine([return_X_y])	Load and return the wine dataset (classification).
datasets.mldata_filename(dataname)	Convert a raw name for a data set in a mldata.org filename.
##### 样品生成
datasets.make_biclusters(shape, n_clusters)	Generate an array with constant block diagonal structure for biclustering.
datasets.make_blobs([n_samples, n_features, …])	Generate isotropic Gaussian blobs for clustering.
datasets.make_checkerboard(shape, n_clusters)	Generate an array with block checkerboard structure for biclustering.
datasets.make_circles([n_samples, shuffle, …])	Make a large circle containing a smaller circle in 2d.
datasets.make_classification([n_samples, …])	Generate a random n-class classification problem.
datasets.make_friedman1([n_samples, …])	Generate the “Friedman #1” regression problem
datasets.make_friedman2([n_samples, noise, …])	Generate the “Friedman #2” regression problem
datasets.make_friedman3([n_samples, noise, …])	Generate the “Friedman #3” regression problem
datasets.make_gaussian_quantiles([mean, …])	Generate isotropic Gaussian and label samples by quantile
datasets.make_hastie_10_2([n_samples, …])	Generates data for binary classification used in Hastie et al.
datasets.make_low_rank_matrix([n_samples, …])	Generate a mostly low rank matrix with bell-shaped singular values
datasets.make_moons([n_samples, shuffle, …])	Make two interleaving half circles
datasets.make_multilabel_classification([…])	Generate a random multilabel classification problem.
datasets.make_regression([n_samples, …])	Generate a random regression problem.
datasets.make_s_curve([n_samples, noise, …])	Generate an S curve dataset.
datasets.make_sparse_coded_signal(n_samples, …)	Generate a signal as a sparse combination of dictionary elements.
datasets.make_sparse_spd_matrix([dim, …])	Generate a sparse symmetric definite positive matrix.
datasets.make_sparse_uncorrelated([…])	Generate a random regression problem with sparse uncorrelated design
datasets.make_spd_matrix(n_dim[, random_state])	Generate a random symmetric, positive-definite matrix.
datasets.make_swiss_roll([n_samples, noise, …])	Generate a swiss roll dataset.
sklearn.decomposition: 矩阵分解
The sklearn.decomposition module includes matrix decomposition algorithms, including among others PCA, NMF or ICA. Most of the algorithms of this module can be regarded as dimensionality reduction techniques.

用户指南: 参阅 分解成分中的信号（矩阵分解问题） 章节获取更多内容。

decomposition.DictionaryLearning([…])	Dictionary learning
decomposition.FactorAnalysis([n_components, …])	Factor Analysis (FA)
decomposition.FastICA([n_components, …])	FastICA: a fast algorithm for Independent Component Analysis.
decomposition.IncrementalPCA([n_components, …])	Incremental principal components analysis (IPCA).
decomposition.KernelPCA([n_components, …])	Kernel Principal component analysis (KPCA)
decomposition.LatentDirichletAllocation([…])	Latent Dirichlet Allocation with online variational Bayes algorithm
decomposition.MiniBatchDictionaryLearning([…])	Mini-batch dictionary learning
decomposition.MiniBatchSparsePCA([…])	Mini-batch Sparse Principal Components Analysis
decomposition.NMF([n_components, init, …])	Non-Negative Matrix Factorization (NMF)
decomposition.PCA([n_components, copy, …])	Principal component analysis (PCA)
decomposition.SparsePCA([n_components, …])	Sparse Principal Components Analysis (SparsePCA)
decomposition.SparseCoder(dictionary[, …])	Sparse coding
decomposition.TruncatedSVD([n_components, …])	Dimensionality reduction using truncated SVD (aka LSA).
decomposition.dict_learning(X, n_components, …)	Solves a dictionary learning matrix factorization problem.
decomposition.dict_learning_online(X[, …])	Solves a dictionary learning matrix factorization problem online.
decomposition.fastica(X[, n_components, …])	Perform Fast Independent Component Analysis.
decomposition.sparse_encode(X, dictionary[, …])	Sparse coding

##### sklearn.discriminant_analysis: 判别分析

Linear Discriminant Analysis and Quadratic Discriminant Analysis

用户指南: 参阅 线性和二次判别分析 章节获取更多内容。

discriminant_analysis.LinearDiscriminantAnalysis([…])	Linear Discriminant Analysis
discriminant_analysis.QuadraticDiscriminantAnalysis([…])	Quadratic Discriminant Analysis

#####  sklearn.dummy: 虚拟估计器
用户指南: 参阅 模型评估: 量化预测的质量 章节获取更多内容。

dummy.DummyClassifier([strategy, …])	DummyClassifier is a classifier that makes predictions using simple rules.
dummy.DummyRegressor([strategy, constant, …])	DummyRegressor is a regressor that makes predictions using simple rules.

#####  sklearn.ensemble: 集成方法

The sklearn.ensemble module includes ensemble-based methods for classification, regression and anomaly detection.

用户指南： 参阅 集成方法 章节获取更多内容。

ensemble.AdaBoostClassifier([…])	An AdaBoost classifier.
ensemble.AdaBoostRegressor([base_estimator, …])	An AdaBoost regressor.
ensemble.BaggingClassifier([base_estimator, …])	A Bagging classifier.
ensemble.BaggingRegressor([base_estimator, …])	A Bagging regressor.
ensemble.ExtraTreesClassifier([…])	An extra-trees classifier.
ensemble.ExtraTreesRegressor([n_estimators, …])	An extra-trees regressor.
ensemble.GradientBoostingClassifier([loss, …])	Gradient Boosting for classification.
ensemble.GradientBoostingRegressor([loss, …])	Gradient Boosting for regression.
ensemble.IsolationForest([n_estimators, …])	Isolation Forest Algorithm
ensemble.RandomForestClassifier([…])	A random forest classifier.
ensemble.RandomForestRegressor([…])	A random forest regressor.
ensemble.RandomTreesEmbedding([…])	An ensemble of totally random trees.
ensemble.VotingClassifier(estimators[, …])	Soft Voting/Majority Rule classifier for unfitted estimators.
部分依赖
Partial dependence plots for tree ensembles.

ensemble.partial_dependence.partial_dependence(…)	Partial dependence of target_variables.
ensemble.partial_dependence.plot_partial_dependence(…)	Partial dependence plots for features.
#####  sklearn.exceptions: 异常和警告

The sklearn.exceptions module includes all custom warnings and error classes used across scikit-learn.

exceptions.ChangedBehaviorWarning	Warning class used to notify the user of any change in the behavior.
exceptions.ConvergenceWarning	Custom warning to capture convergence problems
exceptions.DataConversionWarning	Warning used to notify implicit data conversions happening in the code.
exceptions.DataDimensionalityWarning	Custom warning to notify potential issues with data dimensionality.
exceptions.EfficiencyWarning	Warning used to notify the user of inefficient computation.
exceptions.FitFailedWarning	Warning class used if there is an error while fitting the estimator.
exceptions.NotFittedError	Exception class to raise if estimator is used before fitting.
exceptions.NonBLASDotWarning	Warning used when the dot operation does not use BLAS.
exceptions.UndefinedMetricWarning	Warning used when the metric is invalid

#####   sklearn.feature_extraction: 特征提取

The sklearn.feature_extraction module deals with feature extraction from raw data. It currently includes methods to extract features from text and images.

用户指南： 参阅 特征提取 章节获取更多内容。

feature_extraction.DictVectorizer([dtype, …])	Transforms lists of feature-value mappings to vectors.
feature_extraction.FeatureHasher([…])	Implements feature hashing, aka the hashing trick.
图像特征提取
The sklearn.feature_extraction.image submodule gathers utilities to extract features from images.

feature_extraction.image.extract_patches_2d(…)	Reshape a 2D image into a collection of patches
feature_extraction.image.grid_to_graph(n_x, n_y)	Graph of the pixel-to-pixel connections
feature_extraction.image.img_to_graph(img[, …])	Graph of the pixel-to-pixel gradient connections
feature_extraction.image.reconstruct_from_patches_2d(…)	Reconstruct the image from all of its patches.
feature_extraction.image.PatchExtractor([…])	Extracts patches from a collection of images
文本特征提取
The sklearn.feature_extraction.text submodule gathers utilities to build feature vectors from text documents.

feature_extraction.text.CountVectorizer([…])	Convert a collection of text documents to a matrix of token counts
feature_extraction.text.HashingVectorizer([…])	Convert a collection of text documents to a matrix of token occurrences
feature_extraction.text.TfidfTransformer([…])	Transform a count matrix to a normalized tf or tf-idf representation
feature_extraction.text.TfidfVectorizer([…])	Convert a collection of raw documents to a matrix of TF-IDF features.

#####   sklearn.feature_selection: 特征选择

The sklearn.feature_selection module implements feature selection algorithms. It currently includes univariate filter selection methods and the recursive feature elimination algorithm.

用户指南: 参阅 特征选择 章节获取更多内容。

feature_selection.GenericUnivariateSelect([…])	Univariate feature selector with configurable strategy.
feature_selection.SelectPercentile([…])	Select features according to a percentile of the highest scores.
feature_selection.SelectKBest([score_func, k])	Select features according to the k highest scores.
feature_selection.SelectFpr([score_func, alpha])	Filter: Select the pvalues below alpha based on a FPR test.
feature_selection.SelectFdr([score_func, alpha])	Filter: Select the p-values for an estimated false discovery rate
feature_selection.SelectFromModel(estimator)	Meta-transformer for selecting features based on importance weights.
feature_selection.SelectFwe([score_func, alpha])	Filter: Select the p-values corresponding to Family-wise error rate
feature_selection.RFE(estimator[, …])	Feature ranking with recursive feature elimination.
feature_selection.RFECV(estimator[, step, …])	Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.
feature_selection.VarianceThreshold([threshold])	Feature selector that removes all low-variance features.
feature_selection.chi2(X, y)	Compute chi-squared stats between each non-negative feature and class.
feature_selection.f_classif(X, y)	Compute the ANOVA F-value for the provided sample.
feature_selection.f_regression(X, y[, center])	Univariate linear regression tests.
feature_selection.mutual_info_classif(X, y)	Estimate mutual information for a discrete target variable.
feature_selection.mutual_info_regression(X, y)	Estimate mutual information for a continuous target variable.

#####   sklearn.gaussian_process: 高斯过程

The sklearn.gaussian_process module implements Gaussian Process based regression and classification.

用户指南: 参阅 高斯过程 章节获取更多内容。

gaussian_process.GaussianProcessClassifier([…])	Gaussian process classification (GPC) based on Laplace approximation.
gaussian_process.GaussianProcessRegressor([…])	Gaussian process regression (GPR).
核函数

gaussian_process.kernels.CompoundKernel(kernels)	Kernel which is composed of a set of other kernels.
gaussian_process.kernels.ConstantKernel([…])	Constant kernel.
gaussian_process.kernels.DotProduct([…])	Dot-Product kernel.
gaussian_process.kernels.ExpSineSquared([…])	Exp-Sine-Squared kernel.
gaussian_process.kernels.Exponentiation(…)	Exponentiate kernel by given exponent.
gaussian_process.kernels.Hyperparameter	A kernel hyperparameter’s specification in form of a namedtuple.
gaussian_process.kernels.Kernel	Base class for all kernels.
gaussian_process.kernels.Matern([…])	Matern kernel.
gaussian_process.kernels.PairwiseKernel([…])	Wrapper for kernels in sklearn.metrics.pairwise.
gaussian_process.kernels.Product(k1, k2)	Product-kernel k1 * k2 of two kernels k1 and k2.
gaussian_process.kernels.RBF([length_scale, …])	Radial-basis function kernel (aka squared-exponential kernel).
gaussian_process.kernels.RationalQuadratic([…])	Rational Quadratic kernel.
gaussian_process.kernels.Sum(k1, k2)	Sum-kernel k1 + k2 of two kernels k1 and k2.
gaussian_process.kernels.WhiteKernel([…])	White kernel.

#####  sklearn.isotonic: 保序回归

用户指南; 参阅 等式回归 章节获取更多内容。

isotonic.IsotonicRegression([y_min, y_max, …])	Isotonic regression model.
isotonic.check_increasing(x, y)	Determine whether y is monotonically correlated with x.
isotonic.isotonic_regression(y[, …])	Solve the isotonic regression model:

#####  sklearn.kernel_approximation 核近似

The sklearn.kernel_approximation module implements several approximate kernel feature maps base on Fourier transforms.

用户指南: 参阅 内核近似 获取更多内容。

kernel_approximation.AdditiveChi2Sampler([…])	Approximate feature map for additive chi2 kernel.
kernel_approximation.Nystroem([kernel, …])	Approximate a kernel map using a subset of the training data.
kernel_approximation.RBFSampler([gamma, …])	Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.
kernel_approximation.SkewedChi2Sampler([…])	Approximates feature map of the “skewed chi-squared” kernel by Monte Carlo approximation of its Fourier transform.

#####  sklearn.kernel_ridge 内核岭回归

Module sklearn.kernel_ridge implements kernel ridge regression.

用户指南: 参阅 内核岭回归 章节获取更多内容。

kernel_ridge.KernelRidge([alpha, kernel, …])	Kernel ridge regression.

#####  sklearn.linear_model: 广义线性模型

The sklearn.linear_model module implements generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.

用户指南: 参阅 广义线性模型 章节获取更多内容。

linear_model.ARDRegression([n_iter, tol, …])	Bayesian ARD regression.
linear_model.BayesianRidge([n_iter, tol, …])	Bayesian ridge regression
linear_model.ElasticNet([alpha, l1_ratio, …])	Linear regression with combined L1 and L2 priors as regularizer.
linear_model.ElasticNetCV([l1_ratio, eps, …])	Elastic Net model with iterative fitting along a regularization path
linear_model.HuberRegressor([epsilon, …])	Linear regression model that is robust to outliers.
linear_model.Lars([fit_intercept, verbose, …])	Least Angle Regression model a.k.a.
linear_model.LarsCV([fit_intercept, …])	Cross-validated Least Angle Regression model
linear_model.Lasso([alpha, fit_intercept, …])	Linear Model trained with L1 prior as regularizer (aka the Lasso)
linear_model.LassoCV([eps, n_alphas, …])	Lasso linear model with iterative fitting along a regularization path
linear_model.LassoLars([alpha, …])	Lasso model fit with Least Angle Regression a.k.a.
linear_model.LassoLarsCV([fit_intercept, …])	Cross-validated Lasso, using the LARS algorithm
linear_model.LassoLarsIC([criterion, …])	Lasso model fit with Lars using BIC or AIC for model selection
linear_model.LinearRegression([…])	Ordinary least squares Linear Regression.
linear_model.LogisticRegression([penalty, …])	Logistic Regression (aka logit, MaxEnt) classifier.
linear_model.LogisticRegressionCV([Cs, …])	Logistic Regression CV (aka logit, MaxEnt) classifier.
linear_model.MultiTaskLasso([alpha, …])	Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer
linear_model.MultiTaskElasticNet([alpha, …])	Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer
linear_model.MultiTaskLassoCV([eps, …])	Multi-task L1/L2 Lasso with built-in cross-validation.
linear_model.MultiTaskElasticNetCV([…])	Multi-task L1/L2 ElasticNet with built-in cross-validation.
linear_model.OrthogonalMatchingPursuit([…])	Orthogonal Matching Pursuit model (OMP)
linear_model.OrthogonalMatchingPursuitCV([…])	Cross-validated Orthogonal Matching Pursuit model (OMP)
linear_model.PassiveAggressiveClassifier([…])	Passive Aggressive Classifier
linear_model.PassiveAggressiveRegressor([C, …])	Passive Aggressive Regressor
linear_model.Perceptron([penalty, alpha, …])	Read more in the User Guide.
linear_model.RANSACRegressor([…])	RANSAC (RANdom SAmple Consensus) algorithm.
linear_model.Ridge([alpha, fit_intercept, …])	Linear least squares with l2 regularization.
linear_model.RidgeClassifier([alpha, …])	Classifier using Ridge regression.
linear_model.RidgeClassifierCV([alphas, …])	Ridge classifier with built-in cross-validation.
linear_model.RidgeCV([alphas, …])	Ridge regression with built-in cross-validation.
linear_model.SGDClassifier([loss, penalty, …])	Linear classifiers (SVM, logistic regression, a.o.) with SGD training.
linear_model.SGDRegressor([loss, penalty, …])	Linear model fitted by minimizing a regularized empirical loss with SGD
linear_model.TheilSenRegressor([…])	Theil-Sen Estimator: robust multivariate regression model.
linear_model.enet_path(X, y[, l1_ratio, …])	Compute elastic net path with coordinate descent
linear_model.lars_path(X, y[, Xy, Gram, …])	Compute Least Angle Regression or Lasso path using LARS algorithm [1]
linear_model.lasso_path(X, y[, eps, …])	Compute Lasso path with coordinate descent
linear_model.lasso_stability_path(*args, …)	DEPRECATED: The function lasso_stability_path is deprecated in 0.19 and will be removed in 0.21.
linear_model.logistic_regression_path(X, y)	Compute a Logistic Regression model for a list of regularization parameters.
linear_model.orthogonal_mp(X, y[, …])	Orthogonal Matching Pursuit (OMP)
linear_model.orthogonal_mp_gram(Gram, Xy[, …])	Gram Orthogonal Matching Pursuit (OMP)

#####  sklearn.manifold: 集成学习

The sklearn.manifold module implements data embedding techniques.

用户指南: 参阅 流形学习 章节获取更多内容。

manifold.Isomap([n_neighbors, n_components, …])	Isomap Embedding
manifold.LocallyLinearEmbedding([…])	Locally Linear Embedding
manifold.MDS([n_components, metric, n_init, …])	Multidimensional scaling
manifold.SpectralEmbedding([n_components, …])	Spectral embedding for non-linear dimensionality reduction.
manifold.TSNE([n_components, perplexity, …])	t-distributed Stochastic Neighbor Embedding.
manifold.locally_linear_embedding(X, …[, …])	Perform a Locally Linear Embedding analysis on the data.
manifold.smacof(dissimilarities[, metric, …])	Computes multidimensional scaling using the SMACOF algorithm.
manifold.spectral_embedding(adjacency[, …])	Project the sample on the first eigenvectors of the graph Laplacian.

#####  sklearn.metrics: 指标

参阅 模型评估: 量化预测的质量 和 成对的矩阵, 类别和核函数 章节的用户指南获取更多信息。

The sklearn.metrics module includes score functions, performance metrics and pairwise metrics and distance computations.

模型选择接口
参阅 scoring 参数: 定义模型评估规则 章节获取更多内容。

metrics.get_scorer(scoring)	Get a scorer from string
metrics.make_scorer(score_func[, …])	Make a scorer from a performance metric or loss function.
分类指标
参阅 分类指标 用户指南章节获取更多内容。

metrics.accuracy_score(y_true, y_pred[, …])	Accuracy classification score.
metrics.auc(x, y[, reorder])	Compute Area Under the Curve (AUC) using the trapezoidal rule
metrics.average_precision_score(y_true, y_score)	Compute average precision (AP) from prediction scores
metrics.brier_score_loss(y_true, y_prob[, …])	Compute the Brier score.
metrics.classification_report(y_true, y_pred)	Build a text report showing the main classification metrics
metrics.cohen_kappa_score(y1, y2[, labels, …])	Cohen’s kappa: a statistic that measures inter-annotator agreement.
metrics.confusion_matrix(y_true, y_pred[, …])	Compute confusion matrix to evaluate the accuracy of a classification
metrics.dcg_score(y_true, y_score[, k])	Discounted cumulative gain (DCG) at rank K.
metrics.f1_score(y_true, y_pred[, labels, …])	Compute the F1 score, also known as balanced F-score or F-measure
metrics.fbeta_score(y_true, y_pred, beta[, …])	Compute the F-beta score
metrics.hamming_loss(y_true, y_pred[, …])	Compute the average Hamming loss.
metrics.hinge_loss(y_true, pred_decision[, …])	Average hinge loss (non-regularized)
metrics.jaccard_similarity_score(y_true, y_pred)	Jaccard similarity coefficient score
metrics.log_loss(y_true, y_pred[, eps, …])	Log loss, aka logistic loss or cross-entropy loss.
metrics.matthews_corrcoef(y_true, y_pred[, …])	Compute the Matthews correlation coefficient (MCC)
metrics.ndcg_score(y_true, y_score[, k])	Normalized discounted cumulative gain (NDCG) at rank K.
metrics.precision_recall_curve(y_true, …)	Compute precision-recall pairs for different probability thresholds
metrics.precision_recall_fscore_support(…)	Compute precision, recall, F-measure and support for each class
metrics.precision_score(y_true, y_pred[, …])	Compute the precision
metrics.recall_score(y_true, y_pred[, …])	Compute the recall
metrics.roc_auc_score(y_true, y_score[, …])	Compute Area Under the Curve (AUC) from prediction scores
metrics.roc_curve(y_true, y_score[, …])	Compute Receiver operating characteristic (ROC)
metrics.zero_one_loss(y_true, y_pred[, …])	Zero-one classification loss.
回归度量
参阅 回归指标 用户指南章节获取更多内容。

metrics.explained_variance_score(y_true, y_pred)	Explained variance regression score function
metrics.mean_absolute_error(y_true, y_pred)	Mean absolute error regression loss
metrics.mean_squared_error(y_true, y_pred[, …])	Mean squared error regression loss
metrics.mean_squared_log_error(y_true, y_pred)	Mean squared logarithmic error regression loss
metrics.median_absolute_error(y_true, y_pred)	Median absolute error regression loss
metrics.r2_score(y_true, y_pred[, …])	R^2 (coefficient of determination) regression score function.
多标签排序指标
参阅 多标签排名指标 用户指南章节获取更多内容。

metrics.coverage_error(y_true, y_score[, …])	Coverage error measure
metrics.label_ranking_average_precision_score(…)	Compute ranking-based average precision
metrics.label_ranking_loss(y_true, y_score)	Compute Ranking loss measure
聚类指标
参阅 聚类性能度量 用户指南章节获取更多内容

The sklearn.metrics.cluster submodule contains evaluation metrics for cluster analysis results. There are two forms of evaluation:

supervised, which uses a ground truth class values for each sample.
unsupervised, which does not and measures the ‘quality’ of the model itself.
metrics.adjusted_mutual_info_score(…)	Adjusted Mutual Information between two clusterings.
metrics.adjusted_rand_score(labels_true, …)	Rand index adjusted for chance.
metrics.calinski_harabaz_score(X, labels)	Compute the Calinski and Harabaz score.
metrics.completeness_score(labels_true, …)	Completeness metric of a cluster labeling given a ground truth.
metrics.fowlkes_mallows_score(labels_true, …)	Measure the similarity of two clusterings of a set of points.
metrics.homogeneity_completeness_v_measure(…)	Compute the homogeneity and completeness and V-Measure scores at once.
metrics.homogeneity_score(labels_true, …)	Homogeneity metric of a cluster labeling given a ground truth.
metrics.mutual_info_score(labels_true, …)	Mutual Information between two clusterings.
metrics.normalized_mutual_info_score(…)	Normalized Mutual Information between two clusterings.
metrics.silhouette_score(X, labels[, …])	Compute the mean Silhouette Coefficient of all samples.
metrics.silhouette_samples(X, labels[, metric])	Compute the Silhouette Coefficient for each sample.
metrics.v_measure_score(labels_true, labels_pred)	V-measure cluster labeling given a ground truth.
双向聚类指标
参阅 Biclustering 评测 用户指南章节获取更多内容。

metrics.consensus_score(a, b[, similarity])	The similarity of two sets of biclusters.
成对度量
参阅 成对的矩阵, 类别和核函数 用户指南章节获取更多内容。

metrics.pairwise.additive_chi2_kernel(X[, Y])	Computes the additive chi-squared kernel between observations in X and Y
metrics.pairwise.chi2_kernel(X[, Y, gamma])	Computes the exponential chi-squared kernel X and Y.
metrics.pairwise.cosine_similarity(X[, Y, …])	Compute cosine similarity between samples in X and Y.
metrics.pairwise.cosine_distances(X[, Y])	Compute cosine distance between samples in X and Y.
metrics.pairwise.distance_metrics()	Valid metrics for pairwise_distances.
metrics.pairwise.euclidean_distances(X[, Y, …])	Considering the rows of X (and Y=X) as vectors, compute the distance matrix between each pair of vectors.
metrics.pairwise.kernel_metrics()	Valid metrics for pairwise_kernels
metrics.pairwise.laplacian_kernel(X[, Y, gamma])	Compute the laplacian kernel between X and Y.
metrics.pairwise.linear_kernel(X[, Y])	Compute the linear kernel between X and Y.
metrics.pairwise.manhattan_distances(X[, Y, …])	Compute the L1 distances between the vectors in X and Y.
metrics.pairwise.pairwise_distances(X[, Y, …])	Compute the distance matrix from a vector array X and optional Y.
metrics.pairwise.pairwise_kernels(X[, Y, …])	Compute the kernel between arrays X and optional array Y.
metrics.pairwise.polynomial_kernel(X[, Y, …])	Compute the polynomial kernel between X and Y:
metrics.pairwise.rbf_kernel(X[, Y, gamma])	Compute the rbf (gaussian) kernel between X and Y:
metrics.pairwise.sigmoid_kernel(X[, Y, …])	Compute the sigmoid kernel between X and Y:
metrics.pairwise.paired_euclidean_distances(X, Y)	Computes the paired euclidean distances between X and Y
metrics.pairwise.paired_manhattan_distances(X, Y)	Compute the L1 distances between the vectors in X and Y.
metrics.pairwise.paired_cosine_distances(X, Y)	Computes the paired cosine distances between X and Y
metrics.pairwise.paired_distances(X, Y[, metric])	Computes the paired distances between X and Y.
metrics.pairwise_distances(X[, Y, metric, …])	Compute the distance matrix from a vector array X and optional Y.
metrics.pairwise_distances_argmin(X, Y[, …])	Compute minimum distances between one point and a set of points.
metrics.pairwise_distances_argmin_min(X, Y)	Compute minimum distances between one point and a set of points.

#####  sklearn.mixture: 高斯混合模型

The sklearn.mixture module implements mixture modeling algorithms.

用户指南: 参阅 高斯混合模型 章节获取更多内容。

mixture.BayesianGaussianMixture([…])	Variational Bayesian estimation of a Gaussian mixture.
mixture.GaussianMixture([n_components, …])	Gaussian Mixture.

#####  sklearn.model_selection: 模型选择

用户指南: 参阅 交叉验证：评估估算器的表现, 调整估计器的超参数 和 学习曲线 章节获取更多内容。

分离器类
model_selection.GroupKFold([n_splits])	K-fold iterator variant with non-overlapping groups.
model_selection.GroupShuffleSplit([…])	Shuffle-Group(s)-Out cross-validation iterator
model_selection.KFold([n_splits, shuffle, …])	K-Folds cross-validator
model_selection.LeaveOneGroupOut()	Leave One Group Out cross-validator
model_selection.LeavePGroupsOut(n_groups)	Leave P Group(s) Out cross-validator
model_selection.LeaveOneOut()	Leave-One-Out cross-validator
model_selection.LeavePOut(p)	Leave-P-Out cross-validator
model_selection.PredefinedSplit(test_fold)	Predefined split cross-validator
model_selection.RepeatedKFold([n_splits, …])	Repeated K-Fold cross validator.
model_selection.RepeatedStratifiedKFold([…])	Repeated Stratified K-Fold cross validator.
model_selection.ShuffleSplit([n_splits, …])	Random permutation cross-validator
model_selection.StratifiedKFold([n_splits, …])	Stratified K-Folds cross-validator
model_selection.StratifiedShuffleSplit([…])	Stratified ShuffleSplit cross-validator
model_selection.TimeSeriesSplit([n_splits, …])	Time Series cross-validator
分离器函数
model_selection.check_cv([cv, y, classifier])	Input checker utility for building a cross-validator
model_selection.train_test_split(*arrays, …)	Split arrays or matrices into random train and test subsets
超参数优化器
model_selection.GridSearchCV(estimator, …)	Exhaustive search over specified parameter values for an estimator.
model_selection.ParameterGrid(param_grid)	Grid of parameters with a discrete number of values for each.
model_selection.ParameterSampler(…[, …])	Generator on parameters sampled from given distributions.
model_selection.RandomizedSearchCV(…[, …])	Randomized search on hyper parameters.
model_selection.fit_grid_point(X, y, …[, …])	Run fit on one set of parameters.
模型验证
model_selection.cross_validate(estimator, X)	Evaluate metric(s) by cross-validation and also record fit/score times.
model_selection.cross_val_predict(estimator, X)	Generate cross-validated estimates for each input data point
model_selection.cross_val_score(estimator, X)	Evaluate a score by cross-validation
model_selection.learning_curve(estimator, X, y)	Learning curve.
model_selection.permutation_test_score(…)	Evaluate the significance of a cross-validated score with permutations
model_selection.validation_curve(estimator, …)	Validation curve.

#####   sklearn.multiclass: 多类和多标签分类

Multiclass and multilabel classification strategies
This module implements multiclass learning algorithms:
one-vs-the-rest / one-vs-all
one-vs-one
error correcting output codes
The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.

All classifiers in scikit-learn implement multiclass classification; you only need to use this module if you want to experiment with custom multiclass strategies.

The one-vs-the-rest meta-classifier also implements a predict_proba method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample will not sum to unity, as they do in the single label case.

用户指南: 参阅 多类和多标签算法 章节获取更多内容。

multiclass.OneVsRestClassifier(estimator[, …])	One-vs-the-rest (OvR) multiclass/multilabel strategy
multiclass.OneVsOneClassifier(estimator[, …])	One-vs-one multiclass strategy
multiclass.OutputCodeClassifier(estimator[, …])	(Error-Correcting) Output-Code multiclass strategy

#####  sklearn.multioutput: 多输出回归和分类

This module implements multioutput regression and classification.

The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.

用户指南: 参阅 多类和多标签算法 章节获取更多内容。

multioutput.ClassifierChain(base_estimator)	A multi-label model that arranges binary classifiers into a chain.
multioutput.MultiOutputRegressor(estimator)	Multi target regression
multioutput.MultiOutputClassifier(estimator)	Multi target classification

#####  sklearn.naive_bayes: 朴素贝叶斯

The sklearn.naive_bayes module implements Naive Bayes algorithms. These are supervised learning methods based on applying Bayes’ theorem with strong (naive) feature independence assumptions.

用户指南: 参阅 朴素贝叶斯 章节获取更多内容。

naive_bayes.BernoulliNB([alpha, binarize, …])	Naive Bayes classifier for multivariate Bernoulli models.
naive_bayes.GaussianNB([priors])	Gaussian Naive Bayes (GaussianNB)
naive_bayes.MultinomialNB([alpha, …])	Naive Bayes classifier for multinomial models

#####  sklearn.neighbors: 最近邻

The sklearn.neighbors module implements the k-nearest neighbors algorithm.

用户指南: 参阅 最近邻 章节获取更多内容。

neighbors.BallTree	BallTree for fast generalized N-point problems
neighbors.DistanceMetric	DistanceMetric class
neighbors.KDTree	KDTree for fast generalized N-point problems
neighbors.KernelDensity([bandwidth, …])	Kernel Density Estimation
neighbors.KNeighborsClassifier([…])	Classifier implementing the k-nearest neighbors vote.
neighbors.KNeighborsRegressor([n_neighbors, …])	Regression based on k-nearest neighbors.
neighbors.LocalOutlierFactor([n_neighbors, …])	Unsupervised Outlier Detection using Local Outlier Factor (LOF)
neighbors.RadiusNeighborsClassifier([…])	Classifier implementing a vote among neighbors within a given radius
neighbors.RadiusNeighborsRegressor([radius, …])	Regression based on neighbors within a fixed radius.
neighbors.NearestCentroid([metric, …])	Nearest centroid classifier.
neighbors.NearestNeighbors([n_neighbors, …])	Unsupervised learner for implementing neighbor searches.
neighbors.kneighbors_graph(X, n_neighbors[, …])	Computes the (weighted) graph of k-Neighbors for points in X
neighbors.radius_neighbors_graph(X, radius)	Computes the (weighted) graph of Neighbors for points in X

#####  sklearn.neural_network: 神经网络模型

The sklearn.neural_network module includes models based on neural networks.

用户指南: 参阅 神经网络模型（有监督） 和 神经网络模型（无监督） 章节获取更多内容。

neural_network.BernoulliRBM([n_components, …])	Bernoulli Restricted Boltzmann Machine (RBM).
neural_network.MLPClassifier([…])	Multi-layer Perceptron classifier.
neural_network.MLPRegressor([…])	Multi-layer Perceptron regressor.

#####  sklearn.pipeline: 管道线

The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators.

pipeline.FeatureUnion(transformer_list[, …])	Concatenates results of multiple transformer objects.
pipeline.Pipeline(steps[, memory])	Pipeline of transforms with a final estimator.
pipeline.make_pipeline(*steps, **kwargs)	Construct a Pipeline from the given estimators.
pipeline.make_union(*transformers, **kwargs)	Construct a FeatureUnion from the given transformers.

#####  sklearn.preprocessing: 预处理和正则化

The sklearn.preprocessing module includes scaling, centering, normalization, binarization and imputation methods.

用户指南: 参阅 预处理数据 章节获取更多内容。

preprocessing.Binarizer([threshold, copy])	Binarize data (set feature values to 0 or 1) according to a threshold
preprocessing.FunctionTransformer([func, …])	Constructs a transformer from an arbitrary callable.
preprocessing.Imputer([missing_values, …])	Imputation transformer for completing missing values.
preprocessing.KernelCenterer	Center a kernel matrix
preprocessing.LabelBinarizer([neg_label, …])	Binarize labels in a one-vs-all fashion
preprocessing.LabelEncoder	Encode labels with value between 0 and n_classes-1.
preprocessing.MultiLabelBinarizer([classes, …])	Transform between iterable of iterables and a multilabel format
preprocessing.MaxAbsScaler([copy])	Scale each feature by its maximum absolute value.
preprocessing.MinMaxScaler([feature_range, copy])	Transforms features by scaling each feature to a given range.
preprocessing.Normalizer([norm, copy])	Normalize samples individually to unit norm.
preprocessing.OneHotEncoder([n_values, …])	Encode categorical integer features using a one-hot aka one-of-K scheme.
preprocessing.PolynomialFeatures([degree, …])	Generate polynomial and interaction features.
preprocessing.QuantileTransformer([…])	Transform features using quantiles information.
preprocessing.RobustScaler([with_centering, …])	Scale features using statistics that are robust to outliers.
preprocessing.StandardScaler([copy, …])	Standardize features by removing the mean and scaling to unit variance
preprocessing.add_dummy_feature(X[, value])	Augment dataset with an additional dummy feature.
preprocessing.binarize(X[, threshold, copy])	Boolean thresholding of array-like or scipy.sparse matrix
preprocessing.label_binarize(y, classes[, …])	Binarize labels in a one-vs-all fashion
preprocessing.maxabs_scale(X[, axis, copy])	Scale each feature to the [-1, 1] range without breaking the sparsity.
preprocessing.minmax_scale(X[, …])	Transforms features by scaling each feature to a given range.
preprocessing.normalize(X[, norm, axis, …])	Scale input vectors individually to unit norm (vector length).
preprocessing.quantile_transform(X[, axis, …])	Transform features using quantiles information.
preprocessing.robust_scale(X[, axis, …])	Standardize a dataset along any axis
preprocessing.scale(X[, axis, with_mean, …])	Standardize a dataset along any axis

#####  sklearn.random_projection: 随机投影

Random Projection transformers

Random Projections are a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes.

The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.

The main theoretical result behind the efficiency of random projection is the Johnson-Lindenstrauss lemma (quoting Wikipedia):

In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.
用户指南: 参阅 随机投影 章节获取更多内容。

random_projection.GaussianRandomProjection([…])	Reduce dimensionality through Gaussian random projection
random_projection.SparseRandomProjection([…])	Reduce dimensionality through sparse random projection
random_projection.johnson_lindenstrauss_min_dim(…)	Find a ‘safe’ number of components to randomly project to

#####  sklearn.semi_supervised 办监督学习

The sklearn.semi_supervised module implements semi-supervised learning algorithms. These algorithms utilized small amounts of labeled data and large amounts of unlabeled data for classification tasks. This module includes Label Propagation.

用户指南: 参阅 半监督学习 章节获取更多内容。

semi_supervised.LabelPropagation([kernel, …])	Label Propagation classifier
semi_supervised.LabelSpreading([kernel, …])	LabelSpreading model for semi-supervised learning

#####  sklearn.svm: 支持向量机

The sklearn.svm module includes Support Vector Machine algorithms.

用户指南: 参阅 支持向量机 章节获取更多内容。

估计器
svm.LinearSVC([penalty, loss, dual, tol, C, …])	Linear Support Vector Classification.
svm.LinearSVR([epsilon, tol, C, loss, …])	Linear Support Vector Regression.
svm.NuSVC([nu, kernel, degree, gamma, …])	Nu-Support Vector Classification.
svm.NuSVR([nu, C, kernel, degree, gamma, …])	Nu Support Vector Regression.
svm.OneClassSVM([kernel, degree, gamma, …])	Unsupervised Outlier Detection.
svm.SVC([C, kernel, degree, gamma, coef0, …])	C-Support Vector Classification.
svm.SVR([kernel, degree, gamma, coef0, tol, …])	Epsilon-Support Vector Regression.
svm.l1_min_c(X, y[, loss, fit_intercept, …])	Return the lowest bound for C such that for C in (l1_min_C, infinity) the model is guaranteed not to be empty.
初级方法
svm.libsvm.cross_validation	Binding of the cross-validation routine (low-level routine)
svm.libsvm.decision_function	Predict margin (libsvm name for this is predict_values)
svm.libsvm.fit	Train the model using libsvm (low-level method)
svm.libsvm.predict	Predict target values of X given a model (low-level method)
svm.libsvm.predict_proba	Predict probabilities

#####  sklearn.tree: 决策树

The sklearn.tree module includes decision tree-based models for classification and regression.

用户指南: 参阅 决策树 章节获取更多内容。

tree.DecisionTreeClassifier([criterion, …])	A decision tree classifier.
tree.DecisionTreeRegressor([criterion, …])	A decision tree regressor.
tree.ExtraTreeClassifier([criterion, …])	An extremely randomized tree classifier.
tree.ExtraTreeRegressor([criterion, …])	An extremely randomized tree regressor.
tree.export_graphviz(decision_tree[, …])	Export a decision tree in DOT format.

#####  sklearn.utils: 效率

The sklearn.utils module includes various utilities.

开发者指南: 参阅 Utilities for Developers 页获取更多内容

utils.as_float_array(X[, copy, force_all_finite])	Converts an array-like to an array of floats.
utils.assert_all_finite(X)	Throw a ValueError if X contains NaN or infinity.
utils.check_X_y(X, y[, accept_sparse, …])	Input validation for standard estimators.
utils.check_array(array[, accept_sparse, …])	Input validation on an array, list, sparse matrix or similar.
utils.check_consistent_length(*arrays)	Check that all arrays have consistent first dimensions.
utils.check_random_state(seed)	Turn seed into a np.random.RandomState instance
utils.class_weight.compute_class_weight(…)	Estimate class weights for unbalanced datasets.
utils.class_weight.compute_sample_weight(…)	Estimate sample weights by class for unbalanced datasets.
utils.estimator_checks.check_estimator
utils.extmath.safe_sparse_dot(a, b[, …])	Dot product that handle the sparse matrix case correctly
utils.indexable(*iterables)	Make arrays indexable for cross-validation.
utils.resample(*arrays, **options)	Resample arrays or sparse matrices in a consistent way
utils.safe_indexing(X, indices)	Return items or rows from X using indices.
utils.shuffle(*arrays, **options)	Shuffle arrays or sparse matrices in a consistent way
utils.sparsefuncs.incr_mean_variance_axis(X, …)	Compute incremental mean and variance along an axix on a CSR or CSC matrix.
utils.sparsefuncs.inplace_column_scale(X, scale)	Inplace column scaling of a CSC/CSR matrix.
utils.sparsefuncs.inplace_row_scale(X, scale)	Inplace row scaling of a CSR or CSC matrix.
utils.sparsefuncs.inplace_swap_row(X, m, n)	Swaps two rows of a CSC/CSR matrix in-place.
utils.sparsefuncs.inplace_swap_column(X, m, n)	Swaps two columns of a CSC/CSR matrix in-place.
utils.sparsefuncs.mean_variance_axis(X, axis)	Compute mean and variance along an axix on a CSR or CSC matrix
utils.validation.check_is_fitted(estimator, …)	Perform is_fitted validation for estimator.
utils.validation.check_symmetric(array[, …])	Make sure that array is 2D, square and symmetric.
utils.validation.column_or_1d(y[, warn])	Ravel column or 1d numpy array, else raises an error
utils.validation.has_fit_parameter(…)	Checks whether the estimator’s fit method supports the given parameter.

#####  最近弃用

0.21将会移除
linear_model.RandomizedLasso(*args, **kwargs)	Randomized Lasso.
linear_model.RandomizedLogisticRegression(…)	Randomized Logistic Regression
neighbors.LSHForest([n_estimators, radius, …])	Performs approximate nearest neighbor search using LSH forest.
0.20版本将会移除
cross_validation.KFold(n[, n_folds, …])	K-Folds cross validation iterator.
cross_validation.LabelKFold(labels[, n_folds])	K-fold iterator variant with non-overlapping labels.
cross_validation.LeaveOneLabelOut(labels)	Leave-One-Label_Out cross-validation iterator
cross_validation.LeaveOneOut(n)	Leave-One-Out cross validation iterator.
cross_validation.LeavePOut(n, p)	Leave-P-Out cross validation iterator
cross_validation.LeavePLabelOut(labels, p)	Leave-P-Label_Out cross-validation iterator
cross_validation.LabelShuffleSplit(labels[, …])	Shuffle-Labels-Out cross-validation iterator
cross_validation.ShuffleSplit(n[, n_iter, …])	Random permutation cross-validation iterator.
cross_validation.StratifiedKFold(y[, …])	Stratified K-Folds cross validation iterator
cross_validation.StratifiedShuffleSplit(y[, …])	Stratified ShuffleSplit cross validation iterator
cross_validation.PredefinedSplit(test_fold)	Predefined split cross validation iterator
decomposition.RandomizedPCA(*args, **kwargs)	Principal component analysis (PCA) using randomized SVD
gaussian_process.GaussianProcess(*args, **kwargs)	The legacy Gaussian Process model class.
grid_search.ParameterGrid(param_grid)	Grid of parameters with a discrete number of values for each.
grid_search.ParameterSampler(…[, random_state])	Generator on parameters sampled from given distributions.
grid_search.GridSearchCV(estimator, param_grid)	Exhaustive search over specified parameter values for an estimator.
grid_search.RandomizedSearchCV(estimator, …)	Randomized search on hyper parameters.
mixture.DPGMM(*args, **kwargs)	Dirichlet Process Gaussian Mixture Models
mixture.GMM(*args, **kwargs)	Legacy Gaussian Mixture Model
mixture.VBGMM(*args, **kwargs)	Variational Inference for the Gaussian Mixture Model
cross_validation.check_cv(cv[, X, y, classifier])	Input checker utility for building a CV in a user friendly way.
cross_validation.cross_val_predict(estimator, X)	Generate cross-validated estimates for each input data point
cross_validation.cross_val_score(estimator, X)	Evaluate a score by cross-validation
cross_validation.permutation_test_score(…)	Evaluate the significance of a cross-validated score with permutations
cross_validation.train_test_split(*arrays, …)	Split arrays or matrices into random train and test subsets
grid_search.fit_grid_point(X, y, estimator, …)	Run fit on one set of parameters.
learning_curve.learning_curve(estimator, X, y)	Learning curve.
learning_curve.validation_curve(estimator, …)	Validation curve.
